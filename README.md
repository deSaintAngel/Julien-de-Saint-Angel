# ğŸ§  Perceptron Multicouche â€” ImplÃ©mentation Manuelle en Python

Ce projet propose une implÃ©mentation Ã  la main dâ€™un perceptron multicouche (**MLP**, Multi-Layer Perceptron) rÃ©alisÃ©e en Python, sans utiliser de bibliothÃ¨ques de deep learning comme TensorFlow ou PyTorch. L'objectif est de comprendre de maniÃ¨re concrÃ¨te et mathÃ©matique le fonctionnement interne dâ€™un rÃ©seau de neurones, depuis la propagation avant jusqu'Ã  la rÃ©tropropagation du gradient.

## ğŸ“ Structure du projet

Le rÃ©pertoire principal contient les fichiers suivants :

- `network_trainning_perception.py` : Script principal d'entraÃ®nement du rÃ©seau  
- `network_inference_percpetron.py` : Script d'infÃ©rence (prÃ©diction) sur de nouvelles donnÃ©es  
- `data_0.npz` : DonnÃ©es d'entraÃ®nement et de test (contenant `x_train`, `y_train`, etc.)  
- `W_up0.npy`, `W_up1.npy`, etc. : Fichiers de poids du rÃ©seau sauvegardÃ©s aprÃ¨s l'entraÃ®nement

## ğŸ§® PrÃ©sentation du perceptron

Le **perceptron** est une unitÃ© de calcul neuronale qui applique une transformation linÃ©aire suivie d'une fonction d'activation. Formellement, il fonctionne ainsi :

- `z = wÂ·x + b` : combinaison linÃ©aire des entrÃ©es `x` avec des poids `w` et biais `b` (le biais nâ€™est pas utilisÃ© ici)  
- `a = Ïƒ(z)` : application d'une fonction d'activation, ici la **sigmoÃ¯de** : `Ïƒ(z) = 1 / (1 + exp(-z))`

Ce modÃ¨le simple permet de classer des donnÃ©es **linÃ©airement sÃ©parables**. Pour rÃ©soudre des problÃ¨mes plus complexes, on utilise plusieurs couches de perceptrons.

## ğŸ§± RÃ©seaux de neurones multicouches

Le **perceptron multicouche** est un empilement de plusieurs couches de neurones :

- Une **couche d'entrÃ©e** (ex. les pixels d'une image ou un vecteur de caractÃ©ristiques)  
- Une ou plusieurs **couches cachÃ©es** (apprentissage de reprÃ©sentations intermÃ©diaires)  
- Une **couche de sortie** (reprÃ©sentation finale, souvent sous forme de vecteur de classes)

Chaque couche applique la transformation suivante :

- `a(i+1) = Ïƒ(W(i) Â· a(i))`

oÃ¹ :
- `W(i)` est la matrice des poids de la couche `i`  
- `a(i)` est le vecteur dâ€™activations de la couche `i`  
- `Ïƒ` est la fonction dâ€™activation (sigmoÃ¯de ici)

## ğŸ” Apprentissage par rÃ©tropropagation

Lâ€™apprentissage repose sur une boucle en 4 Ã©tapes :

1. **Propagation avant (forward)** : Les donnÃ©es sont passÃ©es Ã  travers toutes les couches pour produire une sortie.  
2. **Calcul de lâ€™erreur** : On compare la sortie rÃ©elle Ã  la sortie attendue (label) via une fonction de perte.  
3. **RÃ©tropropagation du gradient** : Lâ€™erreur est propagÃ©e couche par couche dans le sens inverse afin de calculer les gradients.  
4. **Mise Ã  jour des poids** : Chaque poids est ajustÃ© selon lâ€™algorithme de descente de gradient.

### DÃ©tails mathÃ©matiques

- Fonction dâ€™activation : `Ïƒ(z) = 1 / (1 + exp(-z))`  
- DÃ©rivÃ©e de la sigmoÃ¯de : `Ïƒ'(z) = Ïƒ(z) * (1 - Ïƒ(z))`  
- Calcul du gradient dâ€™erreur sur la couche de sortie : `Î´_L = y - a_L`  
- Propagation de lâ€™erreur dans les couches : `Î´_l = (W_{l+1} Â· Î´_{l+1}) âŠ™ Ïƒ'(a_l)`  
- Mise Ã  jour des poids : `W = W + Î± Â· (a_prev^T Â· Î´)`

oÃ¹ :
- `âŠ™` dÃ©signe le produit Ã©lÃ©ment par Ã©lÃ©ment  
- `Î±` est le taux dâ€™apprentissage

## âš™ï¸ Explication du code

Le code fonctionne sans dÃ©pendances lourdes (juste `numpy`, `matplotlib`, et `tqdm`).

### Initialisation du rÃ©seau

Les poids sont initialisÃ©s alÃ©atoirement pour chaque couche dans la fonction `structure_network0`. Chaque couche reÃ§oit une matrice de poids de taille adaptÃ©e au nombre de neurones.

### EntraÃ®nement

La fonction `trainning` gÃ¨re une passe complÃ¨te dâ€™entraÃ®nement sur une donnÃ©e :
- Calcul des activations pour chaque couche (forward)  
- Calcul de lâ€™erreur finale  
- Propagation des gradients en remontant  
- Mise Ã  jour des poids par la rÃ¨gle de la descente de gradient

Les activations et gradients sont stockÃ©s dans des listes pour chaque couche.

Lâ€™apprentissage global est effectuÃ© sur toutes les donnÃ©es d'entraÃ®nement, via des boucles sur les images (`steps_data`) et des itÃ©rations internes (`steps_in`).

### PrÃ©diction

Le script `network_inference_percpetron.py` applique la fonction `prediction` qui reproduit simplement la propagation avant sur une donnÃ©e de test. On compare ensuite la sortie du rÃ©seau avec le label rÃ©el.

### Ã‰valuation

Le taux de rÃ©ussite est Ã©valuÃ© sur les donnÃ©es de test via la proportion de prÃ©dictions correctes.

Extrait de sortie attendu :

> Le taux de succÃ¨s des prÃ©dictions est de 96.5%

## âœ… Ã€ retenir

- Ce projet met en Å“uvre **tout le mÃ©canisme dâ€™un rÃ©seau de neurones** Ã  la main : propagation, activation, dÃ©rivÃ©es, gradients, mises Ã  jour.  
- Le format choisi (sans bibliothÃ¨ques haut niveau) est idÃ©al pour lâ€™apprentissage et lâ€™expÃ©rimentation.  
- Le modÃ¨le fonctionne sur deux classes (classification binaire), mais peut Ãªtre gÃ©nÃ©ralisÃ©.

## ğŸš€ Pistes dâ€™amÃ©lioration

- Utiliser plus de classes (ex. 10 pour MNIST)  
- Ajouter d'autres fonctions dâ€™activation (ReLU, tanh)  
- Ajouter une rÃ©gularisation (L2)  
- Utiliser des mini-batchs au lieu de traiter une image Ã  la fois  
- Ajouter une interface visuelle pour explorer les prÃ©dictions

## ğŸ‘¤ Auteur

Julien de Saint Angel
* [Julien de SAINT ANGEL](mailto:juliencine17@gmail.com)

